{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COVID-19 Raw Data Processing\n",
    "\n",
    "This file processes one directory of input files based on [available data sets](https://pages.semanticscholar.org/coronavirus-research)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we set up the data directories and experiment information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = '/Users/weale/data/covid/raw/'\n",
    "tmp_dir = '/Users/weale/data/covid/tmp/'\n",
    "output_dir = '/Users/weale/data/covid/out/'\n",
    "\n",
    "experiment = 'comm_use_subset/pdf_json/'\n",
    "\n",
    "#RUN ONCE PER EXPERIMENT DIRECTORY\n",
    "#os.makedirs(tmp_dir + experiment)\n",
    "\n",
    "input_path = input_dir + experiment\n",
    "tmp_path = tmp_dir + experiment\n",
    "output_path = output_dir + experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create file with extracted information\n",
    "\n",
    "Take the original json objects from the given directory and create a single tab-separated output file that contains (per line):\n",
    "1. Paper ID\n",
    "2. Title\n",
    "3. Abstract\n",
    "\n",
    "It also creates a second file that only contains the number of documents in the directory. This will be used to help with array allocation and creation in future steps.\n",
    "\n",
    "This should provide a simple representation for the content, without requiring the full text documents. This helps with rapid prototyping and should be able to run on my laptop without too much duress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "fOUT = open(tmp_path + \"20200420_title_abstract_text.tsv\", \"w\")\n",
    "fCOUNT = open(tmp_path + \"20200420_title_abstract_count.txt\", \"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_len = 0\n",
    "\n",
    "for filename in os.listdir(input_path):\n",
    "    with open(os.path.join(input_path, filename)) as f:\n",
    "        #print(f.name)\n",
    "        \n",
    "        #Load data from the file\n",
    "        parsed_data = json.load(f)\n",
    "        \n",
    "        #Get the ID of the paper\n",
    "        pID = parsed_data['paper_id']\n",
    "\n",
    "        #Get the paper title\n",
    "        tmp = parsed_data['metadata']\n",
    "        if len(tmp) > 0:\n",
    "            title = tmp.get('title')\n",
    "        \n",
    "        #Get the paper abstract\n",
    "        tmp = parsed_data['abstract']\n",
    "        if len(tmp) > 0:\n",
    "            tmp = tmp[0]\n",
    "            abstract = tmp.get('text')\n",
    "\n",
    "        #Combine into the output and print to the file\n",
    "        line = pID + '\\t' + title + '\\t' + abstract + '\\n'\n",
    "        fOUT.write(line)\n",
    "        \n",
    "        #Increment file length\n",
    "        file_len = file_len + 1\n",
    "    \n",
    "    f.close()\n",
    "fOUT.close()\n",
    "\n",
    "## Print the number of elements to another file\n",
    "fCOUNT.write(str(file_len))\n",
    "fCOUNT.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Representation Vectors\n",
    "\n",
    "We will now take the title and abstract information and extract representation vectors using the [spaCY](https://spacy.io/) Natural Language Toolkit and a [scispaCy model](https://github.com/allenai/scispacy).\n",
    "\n",
    "We create an *i* x *j* array for each document, where *i* is the number of tokens in the title/abstract and *j* is the number of elements in the representation vector. For the MEDIUM sized model, *j=200*. We process the titles and abstracts separately.\n",
    "\n",
    "The two resulting arrays are stored as numpy arrays of dimension (*n* x *i* x *j*), where *n* is the number of documents. No further processing is done at this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scispacy\n",
    "import spacy\n",
    "\n",
    "# Using MEDIUM size model from scispacy \n",
    "nlp = spacy.load(\"en_core_sci_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "275\n",
      "1366\n"
     ]
    }
   ],
   "source": [
    "fIN = open(tmp_path + \"20200420_title_abstract_text.tsv\", \"r\")\n",
    "\n",
    "dim_title = 0\n",
    "dim_abstract = 0\n",
    "\n",
    "lines = fIN.readlines()\n",
    "for line in lines:\n",
    "    \n",
    "    elements = line.split('\\t')\n",
    "    \n",
    "    # Use spaCy to extract tokens\n",
    "    len_title = len(nlp(elements[1]))\n",
    "    len_abstract = len(nlp(elements[2]))\n",
    "    \n",
    "    # Find the length of the longest set of title and abstract tokens\n",
    "    if dim_title < len_title:\n",
    "        dim_title = len_title\n",
    "        \n",
    "    if dim_abstract < len_abstract:\n",
    "        dim_abstract = len_abstract\n",
    "\n",
    "fIN.close()\n",
    "\n",
    "# Validation\n",
    "print(dim_title)\n",
    "print(dim_abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9557\n"
     ]
    }
   ],
   "source": [
    "# Load the number of documents\n",
    "\n",
    "fIN = open(tmp_path + \"20200420_title_abstract_count.txt\", \"r\")\n",
    "numlines = int(fIN.readline())\n",
    "fIN.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the arrays for the representation of the titles and abstracts\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "titleArr = np.zeros((numlines, dim_title, 200))\n",
    "abstractArr = np.zeros((numlines, dim_abstract, 200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9557\n"
     ]
    }
   ],
   "source": [
    "fIN = open(tmp_path + \"20200420_title_abstract_text.tsv\", \"r\")\n",
    "\n",
    "i=0\n",
    "lines = fIN.readlines()\n",
    "for line in lines:\n",
    "    elements = line.split('\\t')\n",
    "\n",
    "    processed = nlp(elements[1])\n",
    "    j=0\n",
    "    for token in processed:\n",
    "        titleArr[i,j,:] = token.vector\n",
    "        j+=1\n",
    "        \n",
    "    processed = nlp(elements[2])    \n",
    "    j=0\n",
    "    for token in processed:\n",
    "        abstractArr[i,j,:] = token.vector\n",
    "        j+=1\n",
    "        \n",
    "    i+=1\n",
    "fIN.close()\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the arrays to numpy binary files for future processing\n",
    "\n",
    "from numpy import asarray\n",
    "from numpy import save\n",
    "\n",
    "save(output_path + \"20200420_title_vectors.npy\", titleArr)\n",
    "save(output_path + \"20200420_abstract_vectors.npy\", abstractArr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch] *",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
